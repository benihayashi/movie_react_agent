{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from langchain_community.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>showname</th>\n",
       "      <th>first_airing</th>\n",
       "      <th>imdb</th>\n",
       "      <th>lang</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Allo 'Allo!</td>\n",
       "      <td>1982-12-30</td>\n",
       "      <td>tt0086659</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;In this spoof of World War II, René Artois ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Orrible</td>\n",
       "      <td>2001-09-10</td>\n",
       "      <td>tt0299233</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;Paul Clark is a cab driver and wannabe smal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Run Sbit</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>None</td>\n",
       "      <td>Welsh</td>\n",
       "      <td>&lt;p&gt;Satirical comedy series in a fly on the wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Til Death Do Us Part</td>\n",
       "      <td>2007-03-19</td>\n",
       "      <td>None</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;'Til Death Do Us Part&lt;/b&gt; is murder-myst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Til Death Do Us Part</td>\n",
       "      <td>2019-07-09</td>\n",
       "      <td>tt10553838</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;'Til Death Do Us Part&lt;/b&gt; follows lovers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A Real Bug's Life</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;A Real Bug's Life&lt;/b&gt; is an adventure in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Sister's All You Need.</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>tt9731082</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>&lt;p&gt;This is the story about the daily life of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A Small Light</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>tt17921714</td>\n",
       "      <td>English</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;A Small Light&lt;/b&gt; follows twenty-somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A Soldier's Heart</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>tt12132812</td>\n",
       "      <td>Tagalog</td>\n",
       "      <td>&lt;p&gt;The story of seven individuals who chose th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>A Special Meal of the Weirdo 'Nara'</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>tt6962506</td>\n",
       "      <td>Korean</td>\n",
       "      <td>&lt;p&gt;When you feel low, you must go to the meat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               showname first_airing        imdb      lang  \\\n",
       "0                          'Allo 'Allo!   1982-12-30   tt0086659   English   \n",
       "1                              'Orrible   2001-09-10   tt0299233   English   \n",
       "2                             'Run Sbit   2016-04-01        None     Welsh   \n",
       "3                 'Til Death Do Us Part   2007-03-19        None   English   \n",
       "4                 'Til Death Do Us Part   2019-07-09  tt10553838   English   \n",
       "..                                  ...          ...         ...       ...   \n",
       "95                    A Real Bug's Life         None        None   English   \n",
       "96             A Sister's All You Need.   2017-10-08   tt9731082  Japanese   \n",
       "97                        A Small Light   2023-05-01  tt17921714   English   \n",
       "98                    A Soldier's Heart   2020-01-20  tt12132812   Tagalog   \n",
       "99  A Special Meal of the Weirdo 'Nara'   2017-05-24   tt6962506    Korean   \n",
       "\n",
       "                                          description  \n",
       "0   <p>In this spoof of World War II, René Artois ...  \n",
       "1   <p>Paul Clark is a cab driver and wannabe smal...  \n",
       "2   <p>Satirical comedy series in a fly on the wal...  \n",
       "3   <p><b>'Til Death Do Us Part</b> is murder-myst...  \n",
       "4   <p><b>'Til Death Do Us Part</b> follows lovers...  \n",
       "..                                                ...  \n",
       "95  <p><b>A Real Bug's Life</b> is an adventure in...  \n",
       "96  <p>This is the story about the daily life of a...  \n",
       "97  <p><b>A Small Light</b> follows twenty-somethi...  \n",
       "98  <p>The story of seven individuals who chose th...  \n",
       "99  <p>When you feel low, you must go to the meat ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['showname','first_airing','imdb','lang','description']\n",
    "def fetch_rows(limit: int, offset: int):\n",
    "    conn = sqlite3.connect(\"movie_db.sqlite\")\n",
    "    cursor = conn.cursor()\n",
    "    executor = cursor.execute(f'select {\",\".join(cols)} from tvmaze where metadata is null and description is not null order by showname asc limit {limit} offset {offset}')\n",
    "    data = executor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "pd.DataFrame(fetch_rows(100, 0), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Labeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv('OPENAI_KEY'),\n",
    "    # model=\"deepseek-r1-distill-llama-8b\",\n",
    "    # openai_api_base='http://127.0.0.1:1234/v1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools for extracting metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(values: list[str]):\n",
    "    with open(f'./prompts/movie_metadata_extraction.txt', 'r') as file:\n",
    "        data = file.read().rstrip()\n",
    "        index=0\n",
    "            \n",
    "        while(data.find(f\"!<INPUT_{index}>\") > 0):\n",
    "            data = data.replace(f\"!<INPUT_{index}>\", values[index] if len(values) > index is not None else '')\n",
    "            index+=1\n",
    "        return data\n",
    "\n",
    "    return None;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define job to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def process_job(limit: int, offset: int) -> list[dict]:\n",
    "    results = []\n",
    "    completed = 0\n",
    "    data = fetch_rows(limit, offset)\n",
    "    total_tokens = 0\n",
    "    total_cached_tokens = 0\n",
    "    total_costs = 0\n",
    "    for row in data:\n",
    "        if(row[4] is not None and len(row[4]) > 0):\n",
    "            with get_openai_callback() as cb:  \n",
    "                prompt = generate_prompt([row[4]])\n",
    "                output = llm.invoke([HumanMessage(content=prompt)])\n",
    "                total_tokens+=cb.total_tokens\n",
    "                total_costs+=cb.total_cost\n",
    "                total_cached_tokens+=cb.prompt_tokens_cached\n",
    "                json_str = re.findall(r\"```json(.*?)```\", output.text(),re.DOTALL)\n",
    "                if(len(json_str) > 0):\n",
    "                    try:\n",
    "                        structured_data = json.loads(json_str[0])\n",
    "                        conn = sqlite3.connect(\"movie_db.sqlite\")\n",
    "                        cursor = conn.cursor()\n",
    "                        showname:str = row[0].replace(\"'\", \"\\\\'\")\n",
    "                        cursor.execute(f'update tvmaze set metadata = \\'{json.dumps(structured_data)}\\' where showname = \\'{showname}\\'')\n",
    "                        conn.commit()\n",
    "                        cursor.close()\n",
    "                        conn.close()\n",
    "                        results.append(structured_data)\n",
    "                    except Exception  as e:\n",
    "                        # print(row[0])\n",
    "                        # print(e)\n",
    "                        pass\n",
    "                completed+=1\n",
    "                clear_output(wait=True)\n",
    "                print(f\"{completed}/{limit} done ({round(completed/limit, 2) * 100}%)\")\n",
    "                print(f\"total token use: {total_tokens}\")\n",
    "                print(f\"total cost: ${total_costs}\")\n",
    "                print('-------------')\n",
    "                print(json_str)\n",
    "            \n",
    "    return results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9843/10000 done (98.0%)\n",
      "total token use: 2483823\n",
      "total cost: $0.655994249999999\n",
      "-------------\n",
      "['\\n{\\n  \"locations\": [\"Las Vegas\", \"Beverly Hills\"],\\n  \"characters\": [\"former Las Vegas showgirl\", \"blue-blooded in-laws\"],\\n  \"time period\": [],\\n  \"events\": [\"inheritance\"],\\n  \"sentimental\": [\"widow\"],\\n  \"genre\": [\"comedy\"]\\n}\\n']\n"
     ]
    }
   ],
   "source": [
    "results = process_job(10000,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tools for vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"fake\", base_url='http://127.0.0.1:1234/v1')\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  \n",
    "collection = chroma_client.get_or_create_collection(name=\"movies\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(model=\"text-embedding-nomic-embed-text-v1.5\", input=text)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def db_to_embeddings(limit: int, offset: int):\n",
    "    cols = ['showname','first_airing','imdb','lang','description', 'metadata']\n",
    "    conn = sqlite3.connect(\"movie_db.sqlite\")\n",
    "    cursor = conn.cursor()\n",
    "    executor = cursor.execute(f'select {\",\".join(cols)} from tvmaze where metadata is not null and processed is null order by showname asc limit {limit} offset {offset}')\n",
    "    rows = executor.fetchall()\n",
    "\n",
    "    \n",
    "    for row in rows:\n",
    "        title = row[0]\n",
    "        description=  row[4]\n",
    "        id = f\"{title}-{row[2]}\"\n",
    "        text = f\"{title} {description}\" \n",
    "        language = row[3] \n",
    "\n",
    "        embedding = get_embedding(text)\n",
    "        metadata : dict = json.loads(row[5])\n",
    "        parsed_metadata: dict = {}\n",
    "        \n",
    "        for key in metadata.keys():\n",
    "            value = metadata[key]\n",
    "            if(len(value) == 0):\n",
    "                continue\n",
    "            if(type(metadata[key]) is list):\n",
    "                parsed_metadata[key] = ','.join(value)\n",
    "            if(type(metadata[key]) is int or type(value) is str):\n",
    "                parsed_metadata[key] = value\n",
    "\n",
    "        parsed_metadata[\"title\"] = title if title is not None else ''\n",
    "        parsed_metadata[\"language\"] = language if language is not None else ''\n",
    "        parsed_metadata[\"aired\"] = row[1] if row[1] is not None else ''\n",
    "        print(parsed_metadata)\n",
    "      \n",
    "        collection.add(\n",
    "            ids=[id],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[parsed_metadata]\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f'update tvmaze set processed = true where showname = \\'{title}\\'')\n",
    "        conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 3B no Koibito-None\n",
      "Insert of existing embedding ID: 3B no Koibito-None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'characters': 'Kobayashi Haruka,Shintaro,Yu,Yoshi', 'genre': 'romance,drama', 'title': '3B no Koibito', 'language': 'Japanese', 'aired': '2021-01-09'}\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdb_to_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 45\u001b[0m, in \u001b[0;36mdb_to_embeddings\u001b[1;34m(limit, offset)\u001b[0m\n\u001b[0;32m     38\u001b[0m     collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m     39\u001b[0m         ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mid\u001b[39m],\n\u001b[0;32m     40\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[embedding],\n\u001b[0;32m     41\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39m[parsed_metadata]\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     44\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m---> 45\u001b[0m     executor \u001b[38;5;241m=\u001b[39m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate tvmaze set processed = true where showname = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtitle\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     47\u001b[0m cursor\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mOperationalError\u001b[0m: database is locked"
     ]
    }
   ],
   "source": [
    "db_to_embeddings(10,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 1000 Years of History | Language: English\n",
      "Title: 12 Monkeys | Language: English\n",
      "Title: 14 - Diaries of the Great War | Language: French\n",
      "Title: 1066: A Year to Conquer England | Language: English\n",
      "Title: 11.22.63 | Language: English\n"
     ]
    }
   ],
   "source": [
    "query_text = \"movies about history\"\n",
    "query_embedding = get_embedding(query_text)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for movie in results[\"metadatas\"][0]:\n",
    "    print(f\"Title: {movie['title']} | Language: {movie['language']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
